{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d847910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CreditCardTxnAnalysis\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb6012d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",True).option(\"inferSchema\",True).csv(\"credit_card_transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f5f0c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- txn_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- txn_type: string (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd7ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "166a0e96",
   "metadata": {},
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2521cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "df.select([\n",
    "    count(when(col(c).isNull() | isnan(c), c)).alias(c)\n",
    "    for c in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34c9fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with missing txn_id, user_id, or timestamp\n",
    "df = df.dropna(subset=[\"txn_id\", \"user_id\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f38c6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cast amount to float and filter out -ve amount numbers\n",
    "df = df.withColumn(\"amount\",col(\"amount\").cast(\"float\"))\n",
    "df = df.filter(col(\"amount\")>0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "720a6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert timestamp to timestamp type\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"M/d/yyyy HH:mm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2019615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|timestamp          |\n",
      "+-------------------+\n",
      "|2025-09-02 21:48:00|\n",
      "|2025-09-02 21:46:00|\n",
      "|2025-09-02 21:46:00|\n",
      "|2025-09-02 21:46:00|\n",
      "|2025-09-02 21:45:00|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"timestamp\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc92a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting day, hour and date from timestamp\n",
    "from pyspark.sql.functions import to_date, hour, date_format\n",
    "\n",
    "df = df.withColumn(\"txn_date\", to_date(\"timestamp\"))\n",
    "df = df.withColumn(\"txn_hour\", hour(\"timestamp\"))\n",
    "df = df.withColumn(\"txn_dayofweek\", date_format(\"timestamp\",\"EEEE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff2001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2ecd065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#is weekend column\n",
    "df = df.withColumn(\"is_weekend\", when(df[\"txn_dayofweek\"].isin(\"Saturday\",\"Sunday\"),1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1deb163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create region column\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = df.withColumn(\"region\", split(col(\"location\"), \", \")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8be21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "beb2adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new column for mid-night time transaction\n",
    "df = df.withColumn(\n",
    "\"is_night_txn\",\n",
    "    when(\n",
    "    (col(\"txn_hour\")>=0) & (col(\"txn_hour\")<6),1\n",
    "    ).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be591778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "45a1550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#high value transaction\n",
    "\n",
    "df = df.withColumn(\n",
    "\"is_high_value_txn\",\n",
    "    when(\n",
    "    col(\"amount\")>500,1\n",
    "    ).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ebabf552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill null merchant values with UNKNOWN\n",
    "\n",
    "df = df.fillna({\"merchant\":\"UNKNOWN\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae308cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Spend, Transaction Count, and Fraud Rate per User\n",
    "\n",
    "from pyspark.sql.functions import count, sum, round\n",
    "\n",
    "user_summary = df.groupBy(\"user_id\").agg(\n",
    "    count(\"*\").alias(\"total_txns\"),\n",
    "    round(sum(\"amount\"), 2).alias(\"total_spent\"),\n",
    "    sum(\"is_fraud\").alias(\"fraud_txns\"),\n",
    "    round((sum(\"is_fraud\") / count(\"*\")) * 100, 2).alias(\"fraud_rate_percent\"),\n",
    "    sum(\"is_night_txn\").alias(\"night_txns\"),\n",
    "    sum(\"is_high_value_txn\").alias(\"high_value_txns\")\n",
    ")\n",
    "\n",
    "user_summary.orderBy(\"fraud_rate_percent\", ascending=False).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "37e77a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing with UDFs & Window Functions \n",
    "\n",
    "#Lets create merchant fraud summary table\n",
    "merchant_risk_df = df.groupBy(\"merchant\").agg(\n",
    "    sum(\"is_fraud\").alias(\"fraud_txns\"),\n",
    "    count(\"*\").alias(\"total_txns\")\n",
    ").withColumn(\n",
    "    \"fraud_rate\", round(col(\"fraud_txns\") / col(\"total_txns\"), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check fraud rate\n",
    "merchant_risk_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "39ff7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf's to clasify risk based on fraud_rate\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def classify_merchant(fraud_rate):\n",
    "    if fraud_rate >= 0.05:\n",
    "        return \"High Risk\"\n",
    "    elif fraud_rate >= 0.02:\n",
    "        return \"Moderate Risk\"\n",
    "    else:\n",
    "        return \"Safe\"\n",
    "\n",
    "merchant_risk_udf = udf(classify_merchant, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ae62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_risk_df = merchant_risk_df.withColumn(\n",
    "    \"risk_category\", merchant_risk_udf(col(\"fraud_rate\"))\n",
    ")\n",
    "\n",
    "merchant_risk_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc51688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank Top Spending Users per Region using Window Functions\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"region\").orderBy(col(\"amount\").desc())\n",
    "\n",
    "df_with_rank = df.withColumn(\"spend_rank_in_region\", row_number().over(window_spec))\n",
    "\n",
    "df_with_rank.select(\"user_id\", \"amount\", \"region\", \"spend_rank_in_region\").filter(\"spend_rank_in_region <= 3\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d3e0a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flag possible duplicate transactions\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, unix_timestamp\n",
    "\n",
    "# Only run if timestamp still exists\n",
    "if \"timestamp\" in df.columns:\n",
    "    window_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "\n",
    "    df = df.withColumn(\"prev_txn_time\", lag(\"timestamp\").over(window_spec))\n",
    "    df = df.withColumn(\"time_diff_sec\", \n",
    "                       unix_timestamp(\"timestamp\") - unix_timestamp(\"prev_txn_time\"))\n",
    "    df = df.withColumn(\"is_possible_duplicate\", when(col(\"time_diff_sec\") < 60, 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "70bd4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Aggregate and rank users by total spend per region\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "user_spend = df.groupBy(\"user_id\", \"region\").agg(\n",
    "    sum(\"amount\").alias(\"total_spent\")\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\"region\").orderBy(col(\"total_spent\").desc())\n",
    "\n",
    "top_spenders = user_spend.withColumn(\"rank\", row_number().over(window_spec)).filter(\"rank <= 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "87b396a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame(top_spenders.collect(), columns=top_spenders.columns)\n",
    "pdf.to_excel(\"top_spenders_summary.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2fe9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
